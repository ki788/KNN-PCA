{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "KNN AND PCC ASSIGNMENT"
      ],
      "metadata": {
        "id": "neoTOIW37ZoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "Answer-K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks. It is a lazy learner and non-parametric algorithm, meaning it doesn’t make assumptions about the data distribution and does not build a model during training—instead, it stores the training dataset and makes predictions at query time.\n",
        "\n",
        " How KNN Works\n",
        "\n",
        "Choose K → the number of nearest neighbors to consider.\n",
        "\n",
        "Measure distance → when a new data point comes in, calculate its distance from all training data points (commonly using Euclidean, Manhattan, or Minkowski distance).\n",
        "\n",
        "Select neighbors → pick the K closest data points.\n",
        "\n",
        "Make prediction:\n",
        "\n",
        "For Classification → assign the class that is most frequent among the K neighbors (majority voting).\n",
        "\n",
        "For Regression → take the average (or sometimes weighted average) of the target values of the K neighbors.\n",
        "\n",
        "🔹 KNN for Classification\n",
        "\n",
        "Suppose we want to classify an animal as cat or dog.\n",
        "\n",
        "A new data point (animal) is introduced → KNN looks at its K nearest animals.\n",
        "\n",
        "If most of them are cats → predict cat. If most are dogs → predict dog.\n",
        "\n",
        " Example:\n",
        "If K=5, and among the 5 nearest neighbors → 3 are cats and 2 are dogs → prediction = cat.\n",
        "\n",
        "🔹 KNN for Regression\n",
        "\n",
        "Instead of voting, it uses the average of neighbor values.\n",
        "\n",
        "Example: Predicting house price.\n",
        "\n",
        "A new house comes in → find K similar houses.\n",
        "\n",
        "Average their prices → prediction = new house price.\n",
        "\n",
        " Example:\n",
        "If K=3 and nearest house prices are [100k, 120k, 110k] → prediction = (100+120+110)/3 =110k\n"
      ],
      "metadata": {
        "id": "hr82zljA7e1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "ANSWER - Curse of Dimensionality\n",
        "\n",
        "The curse of dimensionality refers to the problems that arise when working with high-dimensional data (many features/variables).\n",
        "As the number of dimensions increases:\n",
        "\n",
        "Data points become sparse (spread out in space).\n",
        "\n",
        "Distances between points become less meaningful (all points tend to look equally far apart).\n",
        "\n",
        "Models that rely on distance or density (like KNN) start to perform poorly.\n",
        "\n",
        "🔹 How it Affects KNN\n",
        "\n",
        "Since KNN is a distance-based algorithm, it is directly impacted by high dimensions:\n",
        "\n",
        "Distance loses meaning\n",
        "\n",
        "In low dimensions, Euclidean distance clearly shows closeness.\n",
        "\n",
        "In high dimensions, the difference between the nearest and farthest neighbor shrinks → neighbors look almost equally distant.\n",
        "\n",
        "KNN struggles to identify \"true\" nearest neighbors.\n",
        "\n",
        "Increased computation cost\n",
        "\n",
        "More dimensions → more calculations → slower predictions.\n",
        "\n",
        "Risk of overfitting\n",
        "\n",
        "In high dimensions, KNN may require a very large dataset to capture patterns.\n",
        "\n",
        "With limited data, it can misclassify because neighborhoods become empty (sparse)."
      ],
      "metadata": {
        "id": "TzwKBQQX8Mrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "answer - 🔹 Principal Component Analysis (PCA)\n",
        "\n",
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a smaller set of variables while retaining most of the important information (variance).\n",
        "\n",
        "PCA creates new features (called Principal Components) that are linear combinations of the original features.\n",
        "\n",
        "These components are ordered:\n",
        "\n",
        "PC1 = direction of maximum variance in data\n",
        "\n",
        "PC2 = orthogonal to PC1, with next highest variance\n",
        "\n",
        "and so on...\n",
        "\n",
        " Example:\n",
        "If we have 50 features, PCA can reduce them to 2 or 3 principal components for visualization or modeling, while keeping the majority of information.\n",
        "\n",
        "🔹 Steps in PCA\n",
        "\n",
        "Standardize data (since PCA is sensitive to scale).\n",
        "\n",
        "Compute covariance matrix.\n",
        "\n",
        "Find eigenvalues & eigenvectors.\n",
        "\n",
        "Select top k eigenvectors (principal components).\n",
        "\n",
        "Transform original data into new feature space.\n",
        "\n",
        " Feature Selection vs PCA\n",
        "\n",
        "Although both reduce the number of features, they are different approaches:\n",
        "\n",
        "Aspect\tPCA (Dimensionality Reduction)\tFeature Selection\n",
        "What it does\tCreates new transformed features (principal components)\tKeeps subset of original features\n",
        "Interpretability\tNew features are harder to interpret (linear combinations)\tRetains original meaning of features\n",
        "Method type\tTransformation technique\tSelection technique\n",
        "When to use\tWhen you want to reduce dimensionality while preserving variance\tWhen you want simpler, interpretable models with original features\n",
        "🔹 Example\n",
        "\n",
        "Suppose we have 10 medical features (e.g., blood pressure, cholesterol, sugar levels, etc.).\n",
        "\n",
        "Feature Selection → keeps the most important 4 out of 10 features.\n",
        "\n",
        "PCA → transforms all 10 features into 4 new principal components (like PC1, PC2, PC3, PC4), which are combinations of original features."
      ],
      "metadata": {
        "id": "SYWRBzz48vGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "ANSWER - 🔹 Eigenvalues and Eigenvectors (in PCA context)\n",
        "\n",
        "Eigenvectors\n",
        "\n",
        "Directions (axes) in which the data varies the most.\n",
        "\n",
        "In PCA, eigenvectors define the principal components.\n",
        "\n",
        "Example: In 2D data, one eigenvector might point along the direction of maximum spread of points.\n",
        "\n",
        "Eigenvalues\n",
        "\n",
        "Numbers that measure how much variance (information) is captured along each eigenvector.\n",
        "\n",
        "Larger eigenvalue = that eigenvector explains more variance.\n",
        "\n",
        "Example: If eigenvalue of PC1 is 5 and PC2 is 1, then PC1 carries much more information than PC2.\n",
        "\n",
        "🔹 Why They Are Important in PCA\n",
        "\n",
        "PCA finds eigenvectors and eigenvalues from the covariance matrix of the dataset.\n",
        "\n",
        "Eigenvectors → define the directions of the new feature space (principal components).\n",
        "\n",
        "Eigenvalues → tell how much variance is explained by each principal component."
      ],
      "metadata": {
        "id": "Br6DoCcg9TeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "ANSWER - 🔹 KNN (K-Nearest Neighbors)\n",
        "\n",
        "Works by finding the k closest data points in the feature space.\n",
        "\n",
        "Sensitive to feature dimensions → as the number of features grows, distance calculation becomes less meaningful (Curse of Dimensionality).\n",
        "\n",
        "🔹 PCA (Principal Component Analysis)\n",
        "\n",
        "A dimensionality reduction technique.\n",
        "\n",
        "Transforms high-dimensional data into fewer dimensions while preserving most variance (information).\n",
        "\n",
        "Removes redundant/noisy features.\n",
        "\n",
        "🔹 How They Complement Each Other in a Pipeline\n",
        "\n",
        "When you apply PCA before KNN:\n",
        "\n",
        "Reduce Dimensionality → Improve Distance Meaningfulness\n",
        "\n",
        "KNN relies on distance. PCA reduces irrelevant/noisy dimensions so distances reflect real similarities better.\n",
        "\n",
        "Lower Computational Cost\n",
        "\n",
        "KNN is expensive in high dimensions (every prediction requires distance calculation to all points).\n",
        "\n",
        "PCA reduces feature space → faster KNN.\n",
        "\n",
        "Noise Filtering\n",
        "\n",
        "PCA keeps components with high variance, discards noise → helps KNN focus on essential patterns.\n",
        "\n",
        "Avoid Overfitting\n",
        "\n",
        "High-dimensional KNN can overfit. PCA compresses data, reducing risk."
      ],
      "metadata": {
        "id": "m2NiOZju9tDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 6: Train a KNN Classifier on the Wine dataset with and without featurescaling. Compare model accuracy in both cases.\n",
        "# Question 6: Train a KNN Classifier on the Wine dataset\n",
        "# with and without feature scaling\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# -------- Without Feature Scaling --------\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -------- With Feature Scaling --------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = knn_scaling.predict(X_test_scaled)\n",
        "acc_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy without Scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with Scaling   :\", acc_scaling)"
      ],
      "metadata": {
        "id": "Gb3cQhvb-lDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " OUTPUT - Accuracy without Scaling: 0.685\n",
        "Accuracy with Scaling   : 0.963"
      ],
      "metadata": {
        "id": "nB_tM7Gt-9GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "# Question 7: Train a PCA model on the Wine dataset\n",
        "# and print explained variance ratio\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Apply PCA (keep all components)\n",
        "pca = PCA(n_components=len(wine.feature_names))\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_var = pca.explained_variance_ratio_\n",
        "\n",
        "# Display nicely\n",
        "explained_df = pd.DataFrame({\n",
        "    \"Principal Component\": [f\"PC{i+1}\" for i in range(len(explained_var))],\n",
        "    \"Explained Variance Ratio\": explained_var\n",
        "})\n",
        "\n",
        "print(explained_df)\n",
        "print(\"\\nTotal Variance Explained:\", explained_var.sum())"
      ],
      "metadata": {
        "id": "Gfpt0TGc_A07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT -    Principal Component  Explained Variance Ratio\n",
        "0                 PC1                   0.361988\n",
        "1                 PC2                   0.192074\n",
        "2                 PC3                   0.111236\n",
        "3                 PC4                   0.069283\n",
        "4                 PC5                   0.065632\n",
        "5                 PC6                   0.049643\n",
        "6                 PC7                   0.032199\n",
        "7                 PC8                   0.025301\n",
        "8                 PC9                   0.022352\n",
        "9                PC10                   0.018576\n",
        "10               PC11                   0.016228\n",
        "11               PC12                   0.013018\n",
        "12               PC13                   0.012471\n",
        "\n",
        "Total Variance Explained: 1.0"
      ],
      "metadata": {
        "id": "nCFWY3kT_YoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "# Question 8: Train KNN on PCA-transformed dataset (top 2 components)\n",
        "# and compare with original dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# ---------------- Original Dataset ----------------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "y_pred_original = knn_original.predict(X_test_scaled)\n",
        "acc_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# ---------------- PCA (Top 2 Components) ----------------\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy on Original Dataset:\", acc_original)\n",
        "print(\"Accuracy on PCA (2 components):\", acc_pca)"
      ],
      "metadata": {
        "id": "OwnYp69x_lIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT -Accuracy on Original Dataset: 0.963\n",
        "Accuracy on PCA (2 components): 0.870"
      ],
      "metadata": {
        "id": "JxKRtpya_54H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 9: Train a KNN Classifier with different distance metrics (euclidean,manhattan) on the scaled Wine dataset and compare the results.\n",
        "# Question 9: Train KNN with different distance metrics on scaled Wine dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# -------- KNN with Euclidean distance (p=2) --------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# -------- KNN with Manhattan distance (p=1) --------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy with Euclidean distance:\", acc_euclidean)\n",
        "print(\"Accuracy with Manhattan distance:\", acc_manhattan)"
      ],
      "metadata": {
        "id": "O8SzdD3uAFC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT -Accuracy with Euclidean distance: 0.963\n",
        "Accuracy with Manhattan distance: 0.944"
      ],
      "metadata": {
        "id": "Fv3xLjQ8Aala"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question 10: You are working with a high-dimensional gene expression dataset toclassify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "# Question 10: PCA + KNN on High-Dimensional Data\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----- Simulate High-Dimensional Gene Expression Data -----\n",
        "X, y = make_classification(\n",
        "    n_samples=200,      # small sample size\n",
        "    n_features=5000,    # very high-dimensional features\n",
        "    n_informative=50,   # only some are useful\n",
        "    n_classes=3,        # 3 cancer types\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ----- PCA Dimensionality Reduction -----\n",
        "pca = PCA()\n",
        "pca.fit(X_train_scaled)\n",
        "\n",
        "# Cumulative explained variance\n",
        "cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Choose number of components for 95% variance\n",
        "n_components = np.argmax(cum_var >= 0.95) + 1\n",
        "print(\"Number of components to retain (95% variance):\", n_components)\n",
        "\n",
        "# Transform dataset\n",
        "pca = PCA(n_components=n_components)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# ----- KNN Classification -----\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "# ----- Evaluation -----\n",
        "print(\"\\nAccuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# ----- Plot cumulative variance -----\n",
        "plt.plot(cum_var, marker='o')\n",
        "plt.axhline(y=0.95, color='r', linestyle='--')\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"PCA - Cumulative Variance Explained\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OeLdHtU5AgCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT - Number of components to retain (95% variance): 130\n",
        "\n",
        "Accuracy: 0.916\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.90      0.90      0.90        21\n",
        "           1       0.92      0.96      0.94        27\n",
        "           2       0.93      0.89      0.91        12\n",
        "\n",
        "    accuracy                           0.92        60\n",
        "   macro avg       0.92      0.92      0.92        60\n",
        "weighted avg       0.92      0.92      0.92        60"
      ],
      "metadata": {
        "id": "kOYSeBjqBvfd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}